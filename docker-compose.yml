
services:

  cassandra:
    image: cassandra:4
    container_name: cassandra
    restart: always
    ports:
      - "9042:9042"
    volumes:
      - cassandradata:/var/lib/cassandra
    networks:
      - lakehouse-net

  minio:
    image: minio/minio
    container_name: minio
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - miniodata:/data
    networks:
      - lakehouse-net

  dremio:
    image: dremio/dremio-oss
    container_name: dremio
    ports:
      - "9047:9047"
      - "31010:31010"
    networks:
      - lakehouse-net

  airflow-init:
      image: apache/airflow:2.9.1-python3.10
      container_name: airflow-init
      environment:
        AIRFLOW__CORE__EXECUTOR: LocalExecutor
        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@host.docker.internal:5432/airflow
      volumes:
        - ./dags:/opt/airflow/dags
      entrypoint: >
        /bin/bash -c "
          airflow db init &&
          airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
        "
      depends_on:
        - dremio
      networks:
        - lakehouse-net

  airflow-webserver:
    image: apache/airflow:2.9.1-python3.10
    container_name: airflow-webserver
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@host.docker.internal:5432/airflow
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
    command: webserver
    networks:
      - lakehouse-net

  airflow-scheduler:
    image: apache/airflow:2.9.1-python3.10
    container_name: airflow-scheduler
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@host.docker.internal:5432/airflow
    volumes:
      - ./dags:/opt/airflow/dags
    command: scheduler
    networks:
      - lakehouse-net

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - lakehouse-net

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
    networks:
      - lakehouse-net

  airflow-worker-bronze:
    image: apache/airflow:2.8.1-python3.10
    restart: always
    depends_on:
      - airflow-webserver
      - airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./requirements.txt:/requirements.txt
    command: bash -c "pip install -r /requirements.txt && airflow celery worker --queue bronze"
  
  airflow-worker-silver:
    image: apache/airflow:2.8.1-python3.10
    restart: always
    depends_on:
      - airflow-webserver
      - airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./requirements.txt:/requirements.txt
    command: bash -c "pip install -r /requirements.txt && airflow celery worker --queue silver"

  airflow-worker-gold:
    image: apache/airflow:2.8.1-python3.10
    restart: always
    depends_on:
      - airflow-webserver
      - airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./requirements.txt:/requirements.txt
    command: bash -c "pip install -r /requirements.txt && airflow celery worker --queue gold"

volumes:
  cassandradata:
  miniodata:

networks:
  lakehouse-net:
    driver: bridge
